{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DestinyFolder\\DestinyRecaps\\DestinyRecapsApi\\serverproject\n"
     ]
    }
   ],
   "source": [
    "# DIRECTORY SET\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "base_dir=Path(os.getcwd()).parent.parent\n",
    "os.chdir(base_dir)\n",
    "print(os.getcwd())\n",
    "\n",
    "# ENVIRONMENT VARIABLES\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# DJANGO SETUP\n",
    "import django\n",
    "sys.path.append(os.path.abspath(''))\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"serverproject.settings\")\n",
    "django.setup()\n",
    "\n",
    "# Import async modules\n",
    "import asyncio\n",
    "from asgiref.sync import sync_to_async\n",
    "\n",
    "# Import display modules\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Import other modules\n",
    "import faiss\n",
    "\n",
    "# import reloading\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from destinyapp.models import StreamRecapData\n",
    "\n",
    "from destinyapp.customlibrary import services\n",
    "from destinyapp.customlibrary import utils\n",
    "from destinyapp.customlibrary import controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id=\"OqVH_MTBQ6k\"\n",
    "stream_recap_data=await utils.get_recap_data(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db, text_chunks = await services.VectorDbAndTextChunksGenerator.generate_basic_vectordb_and_chunks(video_id, stream_recap_data.transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt=\"Can you explain what the federalist papers are?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results=await services.search(video_id, user_prompt, k_size=10)#, vector_db, text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the overlapping text segments\n",
    "start_stops=[[search_results[\"all_indexes\"][0], search_results[\"all_indexes\"][0]+1000]]\n",
    "for index in search_results[\"all_indexes\"][1:]:\n",
    "    diff_to_last=index-start_stops[-1][1]\n",
    "    if (diff_to_last <= 500) and (diff_to_last >= -500):\n",
    "        if diff_to_last > 0:\n",
    "            start_stops[-1][1]=start_stops[-1][1]+diff_to_last\n",
    "        else:\n",
    "            start_stops[-1][0]=start_stops[-1][0]+diff_to_last\n",
    "        continue\n",
    "\n",
    "    diff_to_last=index-start_stops[-1][0]\n",
    "    if (diff_to_last <= 500) and (diff_to_last >= -500):\n",
    "        if diff_to_last > 0:\n",
    "            start_stops[-1][1]=start_stops[-1][1]+diff_to_last\n",
    "        else:\n",
    "            start_stops[-1][0]=start_stops[-1][0]+diff_to_last\n",
    "        continue\n",
    "    \n",
    "    start_stops.append([index, index+1000])\n",
    "\n",
    "# produce the segments as a string\n",
    "rag_context_str=\"\"\n",
    "for i, ss in enumerate(start_stops):\n",
    "    rag_context_str+=f\"Chunk {i}: \"+stream_recap_data.transcript[ss[0]:ss[1]]+\"\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt=\"\"\"You are a stream bot. You engauge with the user with respect to a past livestream.\n",
    "\n",
    "You will be given context from the stream the user is talking about by method of RAG. Do your best to accuracy answer the user's question or engage intelligently given the context of the stream. \n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the recap for the stream you are to be knowledgeable about:\n",
    "{stream_recap}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the RAG context raw from the transcript that is potentially relevant to what the user is saying:\n",
    "{rag_context}\n",
    "\n",
    "--------------------------------------------\n",
    "Always try to be concise in what you are saying and talk about things in the direct context of the stream or stream recap.\n",
    "\n",
    "\"\"\".format(stream_recap=stream_recap_data.recap, rag_context=rag_context_str)\n",
    "\n",
    "full_prompt=[{\"role\":\"system\", \"content\":system_prompt}, {\"role\":\"user\", \"content\":user_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=await utils.async_response_handler(full_prompt, utils.ModelNameEnum.gpt_4o_mini)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamBot:\n",
    "    chat_history=[]\n",
    "\n",
    "    @classmethod\n",
    "    async def answer_user(self, video_id, user_prompt, test=None):\n",
    "\n",
    "        stream_recap_data=await utils.get_recap_data(video_id)\n",
    "\n",
    "        search_results=await services.search(video_id, user_prompt, k_size=10)#, vector_db, text_chunks \n",
    "\n",
    "        # merge the overlapping text segments\n",
    "        start_stops=[[search_results[\"all_indexes\"][0], search_results[\"all_indexes\"][0]+1000]]\n",
    "        for index in search_results[\"all_indexes\"][1:]:\n",
    "            diff_to_last=index-start_stops[-1][1]\n",
    "            if (diff_to_last <= 500) and (diff_to_last >= -500):\n",
    "                if diff_to_last > 0:\n",
    "                    start_stops[-1][1]=start_stops[-1][1]+diff_to_last\n",
    "                else:\n",
    "                    start_stops[-1][0]=start_stops[-1][0]+diff_to_last\n",
    "                continue\n",
    "\n",
    "            diff_to_last=index-start_stops[-1][0]\n",
    "            if (diff_to_last <= 500) and (diff_to_last >= -500):\n",
    "                if diff_to_last > 0:\n",
    "                    start_stops[-1][1]=start_stops[-1][1]+diff_to_last\n",
    "                else:\n",
    "                    start_stops[-1][0]=start_stops[-1][0]+diff_to_last\n",
    "                continue\n",
    "            \n",
    "            start_stops.append([index, index+1000])\n",
    "\n",
    "        # produce the segments as a string\n",
    "        rag_context_str=\"\"\n",
    "        for i, ss in enumerate(start_stops):\n",
    "            rag_context_str+=f\"Chunk {i}: \"+stream_recap_data.transcript[ss[0]:ss[1]]+\"\\n\\n\"\n",
    "        \n",
    "\n",
    "        # Compile prompt\n",
    "        system_prompt=\"\"\"You are a stream bot. You engauge with the user with respect to a past livestream.\n",
    "\n",
    "        You will be given context from the stream the user is talking about by method of RAG. Do your best to accuracy answer the user's question or engage intelligently given the context of the stream. \n",
    "\n",
    "        --------------------------------------------\n",
    "\n",
    "        Here is the recap for the stream you are to be knowledgeable about:\n",
    "        {stream_recap}\n",
    "\n",
    "        --------------------------------------------\n",
    "\n",
    "        Here is the RAG context raw from the transcript that is potentially relevant to what the user is saying:\n",
    "        {rag_context}\n",
    "\n",
    "        --------------------------------------------\n",
    "        Always try to be concise in what you are saying and talk about things in the direct context of the stream or stream recap.\n",
    "\n",
    "        \"\"\".format(stream_recap=stream_recap_data.recap, rag_context=rag_context_str)\n",
    "\n",
    "        if self.chat_history!=[]:\n",
    "            self.chat_history=[{\"role\":\"system\", \"content\":system_prompt}, {\"role\":\"user\", \"content\":user_prompt}]\n",
    "        else:\n",
    "            self.chat_history[0][\"content\"]=system_prompt\n",
    "            self.chat_history.append({\"role\":\"user\", \"content\":user_prompt})\n",
    "\n",
    "        response=await utils.async_response_handler(full_prompt, utils.ModelNameEnum.gpt_4o_mini)\n",
    "        self.chat_history.append({\"role\":\"system\", \"content\":response})\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History: [{'role': 'user', 'content': 'test'}]\n",
      "Cost:  0.0009653999999999999\n"
     ]
    }
   ],
   "source": [
    "chat_history=[{'role': 'user', 'content': 'test'}]\n",
    "video_id=\"OqVH_MTBQ6k\"\n",
    "stream_bot=services.StreamBot()\n",
    "response=await stream_bot.answer_user(chat_history, video_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It looks like you're testing the system! If you have any questions or topics from the recent livestream you'd like to discuss, feel free to ask!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history[-1][\"role\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_bot=services.StreamBot()\n",
    "video_id=\"OqVH_MTBQ6k\"\n",
    "user_prompt=\"Can you explain what the federalist papers are?\"\n",
    "stream_bot.chat_history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt=input(\"User Prompt: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=await stream_bot.answer_user(video_id, user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://127.0.0.1:8000/api/chatbot_response?pin=194&video_id=OqVH_MTBQ6k'\n",
    "# set the chat_history in the body\n",
    "\n",
    "request_body={\"chat_history\":[\"test\"]}\n",
    "\n",
    "response=requests.post(url, json=request_body)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this b'{\"chat_history\": [\"test\"]}' to a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=b'{\"chat_history\": [\"test\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn string to json\n",
    "# test=test.decode(\"utf-8\")\n",
    "json_data=json.loads(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data[\"chat_history\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
