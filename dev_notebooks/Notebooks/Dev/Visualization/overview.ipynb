{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DestinyFolder\\DestinyRecaps\\DestinyRecapsApi\\serverproject\n"
     ]
    }
   ],
   "source": [
    "# DIRECTORY SET\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "base_dir=Path(os.getcwd()).parent.parent.parent\n",
    "os.chdir(base_dir)\n",
    "print(os.getcwd())\n",
    "\n",
    "# ENVIRONMENT VARIABLES\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# DJANGO SETUP\n",
    "import django\n",
    "sys.path.append(os.path.abspath(''))\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"serverproject.settings\")\n",
    "django.setup()\n",
    "\n",
    "# Import async modules\n",
    "import asyncio\n",
    "from asgiref.sync import sync_to_async\n",
    "\n",
    "# Import display modules\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Import other modules\n",
    "import faiss\n",
    "\n",
    "# import reloading\n",
    "from importlib import reload\n",
    "\n",
    "# Enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import custom modules\n",
    "from destinyapp.models import StreamRecapData\n",
    "\n",
    "from core import services\n",
    "from core import utils\n",
    "from core import controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id=\"OqVH_MTBQ6k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  0.007353\n",
      "Cost:  0.00139975\n",
      "Cost:  0.00227225\n",
      "Cost:  0.002322\n",
      "Cost:  0.00236175\n",
      "Cost:  0.0025397500000000003\n",
      "Cost:  0.0024720000000000002\n",
      "Cost:  0.002572\n",
      "Cost:  0.003057\n",
      "Cost:  0.0029372499999999998\n",
      "Cost:  0.0032025\n",
      "Cost:  0.00305025\n",
      "Cost:  0.00166125\n",
      "Cost:  0.0022725\n",
      "Cost:  0.00248225\n",
      "Results:  20\n",
      "Results:  20\n",
      "Results:  20\n",
      "Results:  0 Text Chunks:  20  Returning blank equal to text chunks length\n",
      "Results:  20\n",
      "Results:  20\n",
      "Results:  13 Text Chunks:  20  Returning blank equal to text chunks length\n",
      "Results:  20\n",
      "Results:  20\n",
      "Results:  20\n",
      "Results:  20\n",
      "Results:  20\n",
      "Results:  20\n",
      "Results:  12\n"
     ]
    }
   ],
   "source": [
    "stream_recap_data=await utils.get_recap_data(video_id)\n",
    "\n",
    "annotated_results, major_topics, minor_topics, cost = await services.stream_plot.generate_data(stream_recap_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counters:  [101, 69]\n"
     ]
    }
   ],
   "source": [
    "plot_object, annotated_results, plot_segments, category_locations = await services.stream_plot.process_data(stream_recap_data,  annotated_results, major_topics, minor_topics, video_id)\n",
    "\n",
    "plot_object, cost=await services.stream_plot.annotate_extra(video_id, stream_recap_data, plot_object)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the plot_object to the database\n",
    "from dataclasses import dataclass, asdict, is_dataclass\n",
    "\n",
    "stream_recap_data=await utils.get_recap_data(video_id)\n",
    "stream_recap_data.plot_object=asdict(plot_object)\n",
    "await sync_to_async(stream_recap_data.save)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save plot_object as json to D:\\DestinyFolder\\DestinyRecaps\\DestinyRecapsWeb\\frontend\\src\\apps\\stream_plot\\plot_data.json\n",
    "from dataclasses import dataclass, asdict, is_dataclass\n",
    "from typing import Any\n",
    "import json\n",
    "plot_object_json=json.dumps(asdict(plot_object))\n",
    "with open(r\"D:\\DestinyFolder\\DestinyRecaps\\DestinyRecapsWeb\\frontend\\src\\apps\\stream_plot\\plot_data.json\", \"w\") as f:\n",
    "    f.write(plot_object_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.services.stream_plot import extra_annotations as ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:  Kamala Harris as Potential Democratic Nominee\n"
     ]
    }
   ],
   "source": [
    "i=11\n",
    "response=await ea.recap_segment(plot_object.segments[i].category, plot_object.segments[i].texts)\n",
    "print(\"Category: \", plot_object.segments[i].category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Discussion centers on Kamala Harris as a potential Democratic nominee against Trump. Logistically, she could inherit campaign funds, but ethically, skipping over her as vice president seems wrong. The conversation questions if Biden's age is the sole factor influencing this decision and whether stepping down would benefit Harris politically.\", 0.00012869999999999998)\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://youtu.be/OqVH_MTBQ6k?t=15188\n",
      "15173\n"
     ]
    }
   ],
   "source": [
    "i_test=100\n",
    "print(plot_object.segments[i_test].href)\n",
    "print(plot_object.segments[i_test].start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services.stream_plot.extra_annotations.annotate_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_offset=500\n",
    "\n",
    "tasks=[]\n",
    "for i in range(len(plot_object.segments)):\n",
    "    task_text=None\n",
    "    if plot_object.segments[i].texts!=[]:\n",
    "        start_index=stream_recap_data.transcript.find(plot_object.segments[i].texts[0])\n",
    "        contextualized_start_index=start_index-context_offset\n",
    "        if (contextualized_start_index)<0:\n",
    "            contextualized_start_index=0\n",
    "        task_text=stream_recap_data.transcript[contextualized_start_index:start_index+len(plot_object.segments[i].texts[0])]\n",
    "    tasks.append(services.stream_plot.extra_annotations.annotate_start(stream_recap_data, plot_object.segments[i].category, contextualized_start_index, task_text))\n",
    "start_counts_and_costs=await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Biden's Decision Not to Seek Re-election\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_object.segments[5].category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "57\n",
      "64\n",
      "69\n",
      "71\n",
      "76\n",
      "78\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(plot_object.segments)):\n",
    "    if plot_object.segments[i].category==\"Biden's Decision Not to Seek Re-election\":\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_recap_data.transcript.find(plot_object.segments[69].texts[0])-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\nC: Eris is 59, though. I mean, so she's a little old.\\n\\nB: Yeah.\\n\\nC: You know. God, yeah. I do want to call out any centrist who was like, I'm not going to name any names, of course, but any centrist who was like, Biden, I can't do that. Now you have an opportunity to stick by your guns and condemn Donald Trump and to join the other ticket and to forcefully say, I support the democratic candidate. I think that would be a great idea. And I think everyone who was kind of on the fence, or maybe even they said, I prefer Trump just because of how old Biden is. Maybe now is the time to stick with your guns and your politics, what you say are your politics, and endorse the democratic nominee.\\n\\nDestiny: Yeah.\\n\\nC: What do you think about that?\\n\\nDestiny: True, I asked, oh, I hit that question at the end of my, I did an interview with Tommy or Tom Bill. Y you the impact theory guy? Yeah, I asked him that at the very end. I wanted to get this question. I asked him before. I think it was the secon\",\n",
       " \"d to last question I asked. I was like, hey, just to be clear, if Biden drops out, and I think I asked if Biden drops out, andamaa comes back, will you seriously reconsider? And he did say, I got him to say yes. I would seriously relook at the candidates or exerted. So, yeah, we'll see.\\n\\nC: Play it back. Play back those tapes. Play. Get the clips, get all of them. And honestly, it should be embarrassing for anyone who calls himself a centrist or who is even concerned about the senility of Joe Biden now, to stand behind Trump or even be equivocal about it, demand answers from them and push back. Hard.\\n\\nDestiny: Yeah, real hard. Even if that means he's gotta make.\\n\\nC: Fun us, earn some bridges. Oh, no, not that.\\n\\nB: Well, you're both here because this was a question I had for Stepven earlier, and then we got sidetracked. But again, now that you've been diving deeper into these Supreme Court cases and judicial cases in general, one of the things that peaceco and I constantly talk about, a\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_object.segments[69].texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://youtu.be/OqVH_MTBQ6k?t=10906'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_object.segments[69].href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(task_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m start_index\u001b[38;5;241m=\u001b[39mstream_recap_data\u001b[38;5;241m.\u001b[39mtranscript\u001b[38;5;241m.\u001b[39mfind(plot_object\u001b[38;5;241m.\u001b[39msegments[t_index]\u001b[38;5;241m.\u001b[39mtexts[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      3\u001b[0m task_text\u001b[38;5;241m=\u001b[39mstream_recap_data\u001b[38;5;241m.\u001b[39mtranscript[contextualized_start_index:start_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(plot_object\u001b[38;5;241m.\u001b[39msegments[t_index]\u001b[38;5;241m.\u001b[39mtexts[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m----> 4\u001b[0m real_start_character_index, cost, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m services\u001b[38;5;241m.\u001b[39mstream_plot\u001b[38;5;241m.\u001b[39mextra_annotations\u001b[38;5;241m.\u001b[39mannotate_start(stream_recap_data, plot_object\u001b[38;5;241m.\u001b[39msegments[t_index]\u001b[38;5;241m.\u001b[39mcategory, stream_recap_data\u001b[38;5;241m.\u001b[39mtranscript\u001b[38;5;241m.\u001b[39mfind(plot_object\u001b[38;5;241m.\u001b[39msegments[t_index]\u001b[38;5;241m.\u001b[39mtexts[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m500\u001b[39m, task_text)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "t_index=60\n",
    "start_index=stream_recap_data.transcript.find(plot_object.segments[t_index].texts[0])\n",
    "task_text=stream_recap_data.transcript[contextualized_start_index:start_index+len(plot_object.segments[t_index].texts[0])]\n",
    "real_start_character_index, cost, response=await services.stream_plot.extra_annotations.annotate_start(stream_recap_data, plot_object.segments[t_index].category, stream_recap_data.transcript.find(plot_object.segments[t_index].texts[0])-500, task_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235500\n",
      "o moderate. Why do you accept dogship moderators? Myron has actually been probably one of the best moderators that I think ive ve had for any debate. I think most of the debates I've had where Myron is moderating, I think he heavily agrees with the other side, like in the rolo TMMSi one and the one with the Suli Mon guy or whatever. But I think he've just done a pretty fair job moderating at all of them. I don't think he's ever done anything wacky or stupid or ever. Yeah. Wait. Okay. I just wanted to read this real quick. I'm sorry. Because this up has to do with a judicial review. I guess I'm going to do anything else with Dan in about an hour. Just so you know. I'll be doing that. So I'm going to have to take off soon to go set up. But let's read this real quick first. Federalist number 78, the judiciary department. This would have been written in. These were all written in, what, 1770? What? Six through eight, I think. Oh, wait, 1780s. Oh, I'm sorry. I might have been. This one is 1788. Oh, damn. Oh, shit. I'm sorry. This was way later than I thought. My bad. Okay. All right, here we go. To the people of the state of New York, we proceed now to examine of the judiciary department of the proposed government. So, as a refresher, the federalist papers were a lot of the founding fathers writing to newspapers, making arguments for federalizing the independent colonies and everything to make a cohesive government, and then arguing about what the structure of that government shou\n"
     ]
    }
   ],
   "source": [
    "print(real_start_character_index)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131500, 0.00035130000000000003)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_counts_and_costs[60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
