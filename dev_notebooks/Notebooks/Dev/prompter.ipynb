{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC at E-M  \n",
    "\n",
    "## This is the master prompt engineering notebook for DestinyRecaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORY SET\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "base_dir=Path(os.getcwd()).parent\n",
    "os.chdir(base_dir)\n",
    "\n",
    "import yaml\n",
    "# data path\n",
    "prompt_yaml_path=\"destinyapp/prompter_.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys loaded\n"
     ]
    }
   ],
   "source": [
    "# DJANGO SETUP\n",
    "import django\n",
    "sys.path.append(os.path.abspath(''))\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"serverproject.settings\")\n",
    "django.setup()\n",
    "\n",
    "# Import custom modules\n",
    "from destinyapp import views\n",
    "from destinyapp.models import TranscriptData\n",
    "from destinyapp.api import ServerAiFunctions as saf\n",
    "\n",
    "# Import async modules\n",
    "import asyncio\n",
    "from asgiref.sync import sync_to_async\n",
    "\n",
    "# Import display modules\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Import other modules\n",
    "import faiss\n",
    "\n",
    "# import reloading\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful reusables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sys.modules['destinyapp.api.ServerAiFunctions']\n",
    "import destinyapp.api.ServerAiFunctions as saf\n",
    "reload(saf)\n",
    "\n",
    "async def run_systems(system_functions, input_contexts):\n",
    "    async def run_system(system_function):\n",
    "        tasks=[]\n",
    "        for input_context in input_contexts:\n",
    "            task=system_function(**input_context)\n",
    "            tasks.append(task)\n",
    "\n",
    "        responses=await asyncio.gather(*tasks)\n",
    "        return responses\n",
    "\n",
    "    systems=[]\n",
    "    for system_function in system_functions:\n",
    "        system=run_system(system_function)\n",
    "        systems.append(system)\n",
    "    \n",
    "    system_responses=await asyncio.gather(*systems)\n",
    "\n",
    "    return system_responses\n",
    "\n",
    "# save to prompt yaml\n",
    "def save_to_prompt_yaml(input_prompt_data):\n",
    "    with open(prompt_yaml_path, 'r') as file:\n",
    "        prompt_data=yaml.load(file, Loader=yaml.FullLoader)\n",
    "    prompt_data[\"working\"]=input_prompt_data\n",
    "    with open(prompt_yaml_path, 'w') as file:\n",
    "        yaml.dump(prompt_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC id=E-M\n",
    "### Items\n",
    "- Summary Chunks id=9xq\n",
    "- Recap id=7l.\n",
    "    - Discord\n",
    "    - Website\n",
    "- Hook id=1m3\n",
    "- Chat v1@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in workingaudio/merged_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "download thread finished\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import AudioFileClip, concatenate_audioclips\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "# delete previous audio file with 'raw' in the name\n",
    "# audio_dir_files=os.listdir(\"workingaudio\")\n",
    "# for file_name in audio_dir_files:\n",
    "#     if 'raw' in file_name:\n",
    "#         os.remove(\"workingaudio/\"+file_name)\n",
    "#         break\n",
    "\n",
    "# open destinyspeaking.mp3\n",
    "destiny_speech_path=\"workingaudio/destinyspeaking.mp3\"\n",
    "destiny_speech = AudioFileClip(destiny_speech_path)\n",
    "audio_dir_files=os.listdir(\"workingaudio\")\n",
    "\n",
    "# find the audio file with 'raw' in the name\n",
    "for file_name in audio_dir_files:\n",
    "    if 'raw' in file_name:\n",
    "        youtube_audio_path=\"workingaudio/\"+file_name\n",
    "        break\n",
    "\n",
    "# Concatentate the two audio files\n",
    "youtube_video = AudioFileClip(youtube_audio_path)\n",
    "merged_audio = concatenate_audioclips([destiny_speech, youtube_video])\n",
    "merged_audio.write_audiofile(\"workingaudio/merged_audio.mp3\")\n",
    "print(\"download thread finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in workingaudio/testing_merged_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "# load merged audio and make a new audio file which is only the first 2000 seconds\n",
    "merged_audio = AudioFileClip(\"workingaudio/merged_audio.mp3\")\n",
    "merged_audio = merged_audio.subclip(0, 2000)\n",
    "merged_audio.write_audiofile(\"workingaudio/testing_merged_audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting assembly transcription thread\n",
      "Finished assembly transcription thread\n",
      "Raw Transcript Finished\n"
     ]
    }
   ],
   "source": [
    "yt_id=\"29ixeHeiLZI\"\n",
    "raw_transcript_data=await saf.assembly_transcript_generation(yt_id, \"workingaudio/merged_audio.mp3\")#os.path.join(output_folder,audio_file_name))\n",
    "save_raw_transcript_data={\"raw_transcript_data\": raw_transcript_data}\n",
    "print(\"Raw Transcript Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'looks', 'start': 186890, 'end': 187242, 'confidence': 0.93248, 'speaker': 'A', 'channel': None}\n",
      "578\n",
      "680\n",
      "Destiny starts\n",
      "Finished diarization cutoff\n",
      "updating transcript data\n"
     ]
    }
   ],
   "source": [
    "# transcript_model_data=await saf.grab_transcript_data(yt_id)\n",
    "raw_transcript_data=save_raw_transcript_data[\"raw_transcript_data\"]\n",
    "save_processed_transcripts=await saf.process_raw_transcript(raw_transcript_data, yt_id)\n",
    "save_processed_transcripts[\"raw_transcript_data\"]=raw_transcript_data\n",
    "\n",
    "# save transcript data\n",
    "await saf.save_data(yt_id, save_processed_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting assembly transcription thread\n",
      "Finished assembly transcription thread\n",
      "Raw Transcript Finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'raw_transcript_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw Transcript Finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m transcript_model_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m saf\u001b[38;5;241m.\u001b[39mgrab_transcript_data(yt_id)\n\u001b[1;32m----> 6\u001b[0m raw_transcript_data\u001b[38;5;241m=\u001b[39m\u001b[43mtranscript_model_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_transcript_data\u001b[49m\n\u001b[0;32m      7\u001b[0m save_raw_transcript_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_transcript_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_transcript_data}\n\u001b[0;32m      8\u001b[0m save_processed_transcripts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m saf\u001b[38;5;241m.\u001b[39mprocess_raw_transcript(raw_transcript_data, yt_id)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'raw_transcript_data'"
     ]
    }
   ],
   "source": [
    "yt_id=\"29ixeHeiLZI\"\n",
    "raw_transcript_data=await saf.testing_assembly_transcript_generation(yt_id, \"workingaudio/testing_merged_audio.mp3\")#os.path.join(output_folder,audio_file_name))\n",
    "save_raw_transcript_data={\"raw_transcript_data\": raw_transcript_data}\n",
    "print(\"Raw Transcript Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_id=\"29ixeHeiLZI\"\n",
    "raw_transcript_data=await saf.testing_assembly_transcript_generation(yt_id, \"workingaudio/testing_merged_audio.mp3\")#os.path.join(output_folder,audio_file_name))\n",
    "save_raw_transcript_data={\"raw_transcript_data\": raw_transcript_data}\n",
    "print(\"Raw Transcript Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'looks', 'start': 186955, 'end': 187331, 'confidence': 0.97028, 'speaker': 'A', 'channel': None}\n",
      "587\n",
      "668\n",
      "Destiny starts\n",
      "Finished diarization cutoff\n",
      "creating new transcript data\n"
     ]
    }
   ],
   "source": [
    "# transcript_model_data=await saf.grab_transcript_data(yt_id)\n",
    "raw_transcript_data=save_raw_transcript_data[\"raw_transcript_data\"]\n",
    "save_processed_transcripts=await saf.process_raw_transcript(raw_transcript_data, yt_id)\n",
    "save_processed_transcripts[\"raw_transcript_data\"]=raw_transcript_data\n",
    "\n",
    "# save transcript data\n",
    "await saf.save_data(yt_id, save_processed_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_processed_transcripts_nano=deepcopy(save_processed_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=29ixeHeiLZI\n",
      "[youtube] 29ixeHeiLZI: Downloading webpage\n",
      "[youtube] 29ixeHeiLZI: Downloading ios player API JSON\n",
      "[youtube] 29ixeHeiLZI: Downloading player 81a0fcab\n",
      "[youtube] 29ixeHeiLZI: Downloading m3u8 information\n",
      "[info] 29ixeHeiLZI: Downloading 1 format(s): 251\n",
      "[download] Destination: workingaudio\\raw.webm\n",
      "[download] 100% of  454.23MiB in 00:05:48 at 1.30MiB/s      \n",
      "download thread closed\n"
     ]
    }
   ],
   "source": [
    "# Video Download\n",
    "import yt_dlp\n",
    "from moviepy.editor import AudioFileClip, concatenate_audioclips\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "async def video_download(video_id):#, output_folder, output_name):\n",
    "    \"\"\"Takes a video id, downloads the video from youtube, concatenates the video with a pre-recorded audio file of the target speaker\n",
    "    \n",
    "    This allows targeted diarization of the audio file.\"\"\"\n",
    "\n",
    "    # Download video\n",
    "    def download_video_thread(video_id):\n",
    "        # Set download parameters\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'outtmpl': 'workingaudio/raw'+'.%(ext)s',#os.path.join(output_folder,output_name)+'.%(ext)s',\n",
    "            'age_limit': 21, \n",
    "        }\n",
    "\n",
    "        # delete previous audio file with 'raw' in the name\n",
    "        audio_dir_files=os.listdir(\"workingaudio\")\n",
    "        for file_name in audio_dir_files:\n",
    "            if 'raw' in file_name:\n",
    "                os.remove(\"workingaudio/\"+file_name)\n",
    "                break\n",
    "\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([\"https://www.youtube.com/watch?v=29ixeHeiLZI\"])\n",
    "            # ydl.download(['https://youtu.be/'+video_id])\n",
    "\n",
    "        # open destinyspeaking.mp3\n",
    "        destiny_speech_path=\"workingaudio/destinyspeaking.mp3\"\n",
    "        destiny_speech = AudioFileClip(destiny_speech_path)\n",
    "        audio_dir_files=os.listdir(\"workingaudio\")\n",
    "\n",
    "        # find the audio file with 'raw' in the name\n",
    "        for file_name in audio_dir_files:\n",
    "            if 'raw' in file_name:\n",
    "                youtube_audio_path=\"workingaudio/\"+file_name\n",
    "                break\n",
    "\n",
    "        # Concatentate the two audio files\n",
    "        youtube_video = AudioFileClip(youtube_audio_path)\n",
    "        merged_audio = concatenate_audioclips([destiny_speech, youtube_video])\n",
    "        merged_audio.write_audiofile(\"workingaudio/merged_audio.mp3\")\n",
    "        print(\"download thread finished\")\n",
    "        return\n",
    "\n",
    "    await asyncio.to_thread(download_video_thread, video_id)\n",
    "    print(\"download thread closed\")\n",
    "await video_download(\"29ixeHeiLZI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Chunks 9xq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset saf \n",
    "del sys.modules['destinyapp.api.ServerAiFunctions']\n",
    "import destinyapp.api.ServerAiFunctions as saf\n",
    "reload(saf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from database\n",
    "video_ids=[\"3kJr7ODrwNw\"]\n",
    "all_transcript_model_data=[]\n",
    "for video_id in video_ids:\n",
    "    trancript_model_data=await saf.grab_transcript_data(video_id)\n",
    "    all_transcript_model_data.append(trancript_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trancript_model_data.transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make request to get the data\n",
    "keys=views.keys\n",
    "import requests\n",
    "request_video_id=\"ZBcQEnCDgDg\"\n",
    "url=\"https://destinyrecaps.com/api/view_raw_transcripts\"\n",
    "query_params={\"mra\":keys[\"req_pass\"], \"video_id\":request_video_id}\n",
    "response=requests.get(url, params=query_params)\n",
    "\n",
    "request_raw_transcript_data=response.json()\n",
    "processed_transcript_data=await saf.process_raw_transcript(request_raw_transcript_data[\"response\"], request_video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt=\"\"\"Your purpose is to take a transcript from a youtube streamer named Destiny and give a synopsis of the content and the sentiment/takes of the speaker. Include all of the topics even if they are covered briefly instead of just covering the main topic although you should do that as well. The main topic or seeming focus of the segment and all of the things said or discussed. This should be quite long.\n",
    "        \n",
    "FYI: The transcript is diarized, Destiny should be annotated 'Destiny' with other speaker being a default from the transcription engine like b, c, d ... etc. You may have to use some intuition to figure out what is happening.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_transcript_data[\"transcript\"][0:29000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems=[saf.summarized_segment_generator.generate_summarized_segments]\n",
    "\n",
    "# set config parameters\n",
    "config_parameters=[\n",
    "    {\"model_company\": saf.ModelCompanyEnum.anthropic, \"model_name\": saf.ModelNameEnum.claude_3_5_sonnet},\n",
    "    {\"model_company\": saf.ModelCompanyEnum.openai, \"model_name\": saf.ModelNameEnum.gpt_4o, \"summarization_prompt\": saf.summarized_segment_generator.long_summarization_prompt},\n",
    "]\n",
    "# set inputs\n",
    "input_variants=[\n",
    "    {\"transcript\": processed_transcript_data[\"transcript\"][0:29000]}\n",
    "]\n",
    "\n",
    "# set input contexts\n",
    "input_contexts=[]\n",
    "for input_variant in input_variants:\n",
    "    for config_parameter in config_parameters:\n",
    "        input_contexts.append({**config_parameter, **input_variant})\n",
    "\n",
    "# run systems\n",
    "system_responses=await run_systems(systems, input_contexts)\n",
    "\n",
    "# save to prompt yaml\n",
    "save_to_prompt_yaml(system_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD\n",
    "summarized_segments=await saf.summarized_segment_generator.generate_summarized_segments(processed_transcript_data[\"transcript\"][0:29000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for summarized_segment in summarized_segments:\n",
    "    print(len(summarized_segment[\"summary\"]), len(summarized_segment[\"transcript\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarized_segments[2][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarized_segments[2][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=saf.ModelNameEnum.claude_3_5_sonnet\n",
    "cost=0\n",
    "input_token_count=0\n",
    "output_token_count=0\n",
    "for summarized_segment in summarized_segments:\n",
    "    input_token_count+=len(saf.enc.encode(summarized_segment[\"transcript\"]))\n",
    "    output_token_count+=len(saf.enc.encode(summarized_segment[\"summary\"]))\n",
    "    cost+=saf.calculate_cost(model_name, summarized_segment[\"transcript\"], summarized_segment[\"summary\"])\n",
    "print(cost, input_token_count, output_token_count)\n",
    "print(\"Average input/output per segment: \", input_token_count/len(summarized_segments), output_token_count/len(summarized_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(saf.enc.encode(\"a c\"*2290))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments=len(summarized_segments)\n",
    "cost=saf.calculate_cost(model_name, \"a c\"*2290*num_segments, \"a c\"*640*num_segments)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(saf.enc.encode(summarized_segment[\"summary\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(47*5000/5)*3/(1000*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(47*2000/5)*15/(1000*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(summarized_segment[\"transcript\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import deepcopy\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_summarized_segments=deepcopy(summarized_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(old_summarized_segments[-3][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarized_segment[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap 71."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# production test\n",
    "discord_recaps_to_send=[{\"meta\":\"Test\",\"yt_id\":\"3kJr7ODrwNw\"}]\n",
    "trancript_model_data=await saf.grab_transcript_data(discord_recaps_to_send[0][\"yt_id\"])\n",
    "recap = await saf.meta_summary_generator.generate_meta_summary(trancript_model_data.summarized_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset saf \n",
    "del sys.modules['destinyapp.api.ServerAiFunctions']\n",
    "import destinyapp.api.ServerAiFunctions as saf\n",
    "reload(saf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transcript data for a video id\n",
    "\n",
    "video_ids=[\"3kJr7ODrwNw\"]\n",
    "\n",
    "for video_id in video_ids:\n",
    "    trancript_model_data=await saf.grab_transcript_data(video_id)\n",
    "    saf.meta_summary_geneator.generate_meta_summary\n",
    "    recap=await saf.generate_meta_summary(trancript_model_data.summarized_chunks, video_id)\n",
    "\n",
    "    save_to_prompt_yaml({\"recap\":recap})\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recap = await saf.meta_summary_geneator.generate_meta_summary(trancript_model_data.summarized_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discord_recaps_to_send=[{\"meta\":\"Test\",\"yt_id\":\"3kJr7ODrwNw\"}]\n",
    "print(\"Getting data\")\n",
    "trancript_model_data=await saf.grab_transcript_data(discord_recaps_to_send[0][\"yt_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids=[\"3kJr7ODrwNw\"]\n",
    "all_transcript_model_data=[]\n",
    "for video_id in video_ids:\n",
    "    trancript_model_data=await saf.grab_transcript_data(video_id)\n",
    "    all_transcript_model_data.append(trancript_model_data)\n",
    "\n",
    "systems=[saf.meta_summary_geneator.generate_meta_summary]\n",
    "\n",
    "# set config parameters\n",
    "config_parameters=[\n",
    "    {\"model_company\": saf.ModelCompanyEnum.anthropic, \"model_name\": saf.ModelNameEnum.claude_3_5_sonnet, \"meta_model_prompt\": saf.meta_summary_geneator.html_sytem},\n",
    "    {\"model_company\": saf.ModelCompanyEnum.openai, \"model_name\": saf.ModelNameEnum.gpt_4o, \"meta_model_prompt\": saf.meta_summary_geneator.html_sytem},\n",
    "]\n",
    "# set inputs\n",
    "input_variants=[\n",
    "    {\"summarized_chunks\": trancript_model_data.summarized_chunks}\n",
    "]\n",
    "\n",
    "# set input contexts\n",
    "input_contexts=[]\n",
    "for input_variant in input_variants:\n",
    "    for config_parameter in config_parameters:\n",
    "        input_contexts.append({**config_parameter, **input_variant})\n",
    "\n",
    "# run systems\n",
    "system_responses=await run_systems(systems, input_contexts)\n",
    "\n",
    "# save to prompt yaml\n",
    "save_to_prompt_yaml(system_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_responses[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom in tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trancript_model_data=await saf.grab_transcript_data(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trancript_model_data.summarized_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recap=\"\"\"Here's a comprehensive summary of the topics discussed in the collection of\n",
    "summaries, organized into main topics and smaller details:\n",
    "\n",
    "# Main Topics\n",
    "\n",
    "### 1\\. Content Creation and Streaming\n",
    "\n",
    "  * **Twitch and streaming:** Destiny expresses frustration with Twitch, criticizing the current state of political content on the platform\n",
    "  * **Relationships with other streamers:** Mentions being on good terms with several Twitch streamers\n",
    "  * **Kit (streaming platform):** Discusses Kit's potential to compete with Twitch\n",
    "  * **Collaboration with other creators:** Expresses reluctance due to differences in approach and audience expectations\n",
    "\n",
    "### 2\\. Political Commentary and Debates\n",
    "\n",
    "  * **Criticism of political commentators:** Particularly those who made incorrect predictions about COVID-19\n",
    "  * **Discussion of political extremes:** Potential for extremes to work together, drawing parallels to historical events\n",
    "  * **Science and politics:** Criticizes misrepresentation of scientific facts for political purposes\n",
    "  * **Israel-Palestine conflict:** References to debates and discussions on this topic\n",
    "\n",
    "### 3\\. Technical Issues and Discussions\n",
    "\n",
    "  * **Video file management:** Issues with large video files and potential solutions\n",
    "  * **Russian bot operation analysis:** In-depth discussion of a potential Russian bot operation on Twitter, focusing on error messages and technical aspects\n",
    "\n",
    "### 4\\. Personal Life and Relationships\n",
    "\n",
    "  * **Work-life balance:** Discusses the relationship between career success and personal life\n",
    "  * **Relationship discussions:** Comments on breakups and relationship dynamics\n",
    "\n",
    "# Smaller Details and Brief Mentions\n",
    "\n",
    "  * Upcoming events and travel plans\n",
    "  * Typing speed and tests\n",
    "  * Current events (e.g., protest group damaging Stonehenge)\n",
    "  * Personal stance on issues and desire for fair treatment in online discourse\n",
    "  * Mental state and need for emotional organization\n",
    "  * Brief mentions of historical atrocities (Holocaust, Holodomor, Great Leap Forward)\n",
    "  * Music and cultural gatekeeping\n",
    "  * January 6th event\n",
    "  * Merchandise promotion\n",
    "  * YouTube drama\n",
    "  * News story about a 12-year-old Jewish girl in Paris\n",
    "  * Cocaine-related incident involving a 9-year-old\n",
    "  * Qatar's relationship with Hamas and Al Jazeera\n",
    "  * Douglas Murray's comments on Israel-Hamas conflict\n",
    "  * Age of consent laws and pedophilia (mentioned in confusion)\n",
    "  * Illegal firearm modifications\n",
    "\n",
    "Overall, Destiny's tone throughout these discussions is often critical,\n",
    "skeptical, and analytical. He frequently expresses frustration with\n",
    "misinformation, personal attacks, and what he perceives as inconsistencies in\n",
    "others' arguments or behaviors. He emphasizes the importance of substantive\n",
    "debates and accurate information in public discourse.\n",
    "\"\"\"\n",
    "recap=\"\"\"Here's a comprehensive summary of the topics discussed in the collection of summaries, organized into main topics and smaller details:\n",
    "\n",
    "<h2>Main Topics</h2>\n",
    "\n",
    "<h3>1. Content Creation and Streaming</h3>\n",
    "<ul>\n",
    "  <li><strong>Twitch and streaming:</strong> Destiny expresses frustration with Twitch, criticizing the current state of political content on the platform</li>\n",
    "  <li><strong>Relationships with other streamers:</strong> Mentions being on good terms with several Twitch streamers</li>\n",
    "  <li><strong>Kit (streaming platform):</strong> Discusses Kit's potential to compete with Twitch</li>\n",
    "  <li><strong>Collaboration with other creators:</strong> Expresses reluctance due to differences in approach and audience expectations</li>\n",
    "</ul>\n",
    "\n",
    "<h3>2. Political Commentary and Debates</h3>\n",
    "<ul>\n",
    "  <li><strong>Criticism of political commentators:</strong> Particularly those who made incorrect predictions about COVID-19</li>\n",
    "  <li><strong>Discussion of political extremes:</strong> Potential for extremes to work together, drawing parallels to historical events</li>\n",
    "  <li><strong>Science and politics:</strong> Criticizes misrepresentation of scientific facts for political purposes</li>\n",
    "  <li><strong>Israel-Palestine conflict:</strong> References to debates and discussions on this topic</li>\n",
    "</ul>\n",
    "\n",
    "<h3>3. Technical Issues and Discussions</h3>\n",
    "<ul>\n",
    "  <li><strong>Video file management:</strong> Issues with large video files and potential solutions</li>\n",
    "  <li><strong>Russian bot operation analysis:</strong> In-depth discussion of a potential Russian bot operation on Twitter, focusing on error messages and technical aspects</li>\n",
    "</ul>\n",
    "\n",
    "<h3>4. Personal Life and Relationships</h3>\n",
    "<ul>\n",
    "  <li><strong>Work-life balance:</strong> Discusses the relationship between career success and personal life</li>\n",
    "  <li><strong>Relationship discussions:</strong> Comments on breakups and relationship dynamics</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Smaller Details and Brief Mentions</h2>\n",
    "\n",
    "<ul>\n",
    "  <li>Upcoming events and travel plans</li>\n",
    "  <li>Typing speed and tests</li>\n",
    "  <li>Current events (e.g., protest group damaging Stonehenge)</li>\n",
    "  <li>Personal stance on issues and desire for fair treatment in online discourse</li>\n",
    "  <li>Mental state and need for emotional organization</li>\n",
    "  <li>Brief mentions of historical atrocities (Holocaust, Holodomor, Great Leap Forward)</li>\n",
    "  <li>Music and cultural gatekeeping</li>\n",
    "  <li>January 6th event</li>\n",
    "  <li>Merchandise promotion</li>\n",
    "  <li>YouTube drama</li>\n",
    "  <li>News story about a 12-year-old Jewish girl in Paris</li>\n",
    "  <li>Cocaine-related incident involving a 9-year-old</li>\n",
    "  <li>Qatar's relationship with Hamas and Al Jazeera</li>\n",
    "  <li>Douglas Murray's comments on Israel-Hamas conflict</li>\n",
    "  <li>Age of consent laws and pedophilia (mentioned in confusion)</li>\n",
    "  <li>Illegal firearm modifications</li>\n",
    "</ul>\n",
    "\n",
    "<p>Overall, Destiny's tone throughout these discussions is often critical, skeptical, and analytical. He frequently expresses frustration with misinformation, personal attacks, and what he perceives as inconsistencies in others' arguments or behaviors. He emphasizes the importance of substantive debates and accurate information in public discourse.</p>\n",
    "\"\"\"\n",
    "\n",
    "# grab which data is associated with each title\n",
    "import re\n",
    "def extract_titles_and_list_items(recap):\n",
    "    title_pattern=re.compile(r'<h3>(.*?)</h3>')\n",
    "    list_item_pattern=re.compile(r'<li>(.*?)</li>')\n",
    "    titles=title_pattern.findall(recap)\n",
    "    list_items=list_item_pattern.findall(recap)\n",
    "    return titles, list_items\n",
    "\n",
    "titles, list_items=extract_titles_and_list_items(recap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_response=await saf.recap_zoomed_in_generator.annotate_zoom_chunks(summaized_chunks=trancript_model_data.summarized_chunks, recap=recap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trancript_model_data.summarized_chunks[8][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_transcript_model_data[0].meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_chunks=\"\"\"Based on the recap and summaries provided, here are recommendations for summary chunks that could be used to create more detailed panes for each of the main topics and smaller details:\n",
    "\n",
    "Large topics:\n",
    "\n",
    "{{{\"Twitch and Streaming\": [0, 5], \"content\": [\"<strong>Twitch and streaming</strong> Destiny discusses his relationship with Twitch, expressing frustration about being unable to respond to slander and threats on the platform. He criticizes the current state of political content on Twitch, saying it has become less collaborative and more focused on individual streamers' clout.\", \"<strong>Twitch and streaming</strong> Destiny mentions being on good terms with several Twitch streamers, including Mizkif, Dr. K, and Asmongold. He emphasizes that he doesn't want blind defense from others, just fair treatment and acknowledgment when he's correct.\"]}}}\n",
    "\n",
    "{{{\"Collaborations and Relationships with Other Creators\": [6, 8], \"content\": [\"<strong>Collaborations and Relationships with Other Creators</strong> Destiny criticizes Hasan's approach to engaging with other content creators, suggesting Hasan only engages when he thinks he can gain clout or handle the situation.\", \"<strong>Collaborations and Relationships with Other Creators</strong> The speaker (Destiny) discusses collaborating with other content creators, expressing reluctance due to differences in approach and audience expectations.\"]}}}\n",
    "\n",
    "{{{\"Personal Stance and Approach\": [0, 1], \"content\": [\"<strong>Personal Stance and Approach</strong> Destiny emphasizes that he speaks out on issues he believes are correct, not to defend specific groups or individuals. He expresses discomfort with being seen as a spokesperson for any particular cause.\", \"<strong>Personal Stance and Approach</strong> Destiny expresses frustration with personal attacks that aren't accompanied by substantive arguments. He's open to criticism of his ideas but dislikes baseless personal attacks.\"]}}}\n",
    "\n",
    "{{{\"Current Events and Debates\": [0, 1, 5], \"content\": [\"<strong>Current Events and Debates</strong> He mentions a protest group damaging Stonehenge with orange spray paint.\", \"<strong>Current Events and Debates</strong> Destiny discusses leveraging his mainstream credibility from large debates. He expresses frustration with a particular individual who initially claimed willingness to debate but later backtracked.\", \"<strong>Current Events and Debates</strong> Reference to a debate or discussion about the Israel-Palestine conflict, with someone challenging Destiny's knowledge on the topic.\"]}}}\n",
    "\n",
    "{{{\"Media and Misinformation\": [1, 4, 5], \"content\": [\"<strong>Media and Misinformation</strong> He's triggered by ignorant people spreading misinformation in large spaces.\", \"<strong>Media and Misinformation</strong> This transcript covers several topics and sentiments expressed by the speaker, Destiny: An alleged exposure of Russian bots on Twitter supporting Trump, which Destiny is skeptical about.\", \"<strong>Media and Misinformation</strong> Criticism of media reporting on unsubstantiated rumors and its potential to cause division.\"]}}}\n",
    "\n",
    "{{{\"Israel-Palestine Conflict\": [5, 6], \"content\": [\"<strong>Israel-Palestine Conflict</strong> Reference to a debate or discussion about the Israel-Palestine conflict, with someone challenging Destiny's knowledge on the topic.\", \"<strong>Israel-Palestine Conflict</strong> Discussion about Hassan Piker's stance on Israel and Palestine, with Destiny questioning Hassan's consistency and motives.\"]}}}\n",
    "\n",
    "{{{\"Video File Management\": [0], \"content\": [\"<strong>Video File Management</strong> Destiny discusses issues with large video files from his cameras and explores options for compressing or managing these files for his editor in Australia. He goes into technical details about file sizes, upload speeds, and potential solutions.\"]}}}\n",
    "\n",
    "{{{\"Russian Bot Analysis\": [3, 4, 5], \"content\": [\"<strong>Russian Bot Analysis</strong> An in-depth analysis of the alleged Russian bot exposure, focusing on the technical aspects of how such a bot might function and why the reported error seems suspicious.\", \"<strong>Russian Bot Analysis</strong> Destiny expresses skepticism about its authenticity.\", \"<strong>Russian Bot Analysis</strong> Discussion about a Twitter bot that was allegedly exposed as a Russian disinformation campaign. The speaker (Destiny) is skeptical of the claims and suggests that the account suspension could be for various reasons.\"]}}}\n",
    "\n",
    "Smaller topics:\n",
    "\n",
    "{{{\"Schedule and Travel\": 0}}}\n",
    "{{{\"Typing Speed\": 0}}}\n",
    "{{{\"Mental State\": 1}}}\n",
    "{{{\"Work-Life Balance\": 2}}}\n",
    "{{{\"Music and Cultural Gatekeeping\": 2}}}\n",
    "{{{\"Relationship Discussions\": 2}}}\n",
    "{{{\"January 6th Event\": 2}}}\n",
    "{{{\"Merchandise Promotion\": 8}}}\n",
    "{{{\"Basketball Game Scene\": 8}}}\n",
    "{{{\"Courtroom and Prison Scenario\": 8}}}\n",
    "{{{\"Illegal Firearm Modifications\": 8}}}\n",
    "{{{\"Personal Interactions\": 8}}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the annotated_chunks\n",
    "import re\n",
    "def parse_annotated_chunks(annotated_chunks):\n",
    "    # find everything\n",
    "    pattern=re.compile(r'\\{\\{\\{.*?\\}\\}\\}')\n",
    "    chunks=pattern.findall(annotated_chunks)\n",
    "\n",
    "    # replace the triple curly braces with single curly braces\n",
    "    chunks=[chunk.replace(\"{{{\", \"{\").replace(\"}}}\", \"}\") for chunk in chunks]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annotated_chunks=parse_annotated_chunks(annotated_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_annotated_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_prompt_str=\"\"\n",
    "annotated_chunk_dict=json.loads(test_annotated_chunks[0])\n",
    "chunk_indexes=[]\n",
    "for key, value in annotated_chunk_dict.items():\n",
    "    if key==\"content\":\n",
    "        topic_prompt_str+=\"\\n\"+\", \".join(value)\n",
    "    else:\n",
    "        chunk_indexes=value\n",
    "        topic_prompt_str+=key\n",
    "\n",
    "transcript_chunks_str=\"\"\n",
    "for chunk_index in chunk_indexes:\n",
    "    transcript_chunks_str+=trancript_model_data.summarized_chunks[chunk_index][\"transcript\"]+\"\\n\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_annotations_str=test_response\n",
    "summarized_chunks=trancript_model_data.summarized_chunks\n",
    "\n",
    "pattern=re.compile(r'\\{\\{\\{.*?\\}\\}\\}')\n",
    "chunk_annotations_temp=pattern.findall(chunk_annotations_str)\n",
    "chunk_annotations_temp=[chunk_annotation.replace(\"{{{\", \"{\").replace(\"}}}\", \"}\") for chunk_annotation in chunk_annotations_temp]\n",
    "chunk_annotations=[]\n",
    "for chunk_annotation in chunk_annotations_temp:\n",
    "    chunk_annotations.append(json.loads(chunk_annotation))\n",
    "\n",
    "# turn summarized chunks into prompt contexts\n",
    "topic_prompts=[]\n",
    "for chunk_annotation in chunk_annotations:\n",
    "    # get topic prompt and chunk indexes for transcript prompt\n",
    "    temp_prompt_str=\"\"\n",
    "    chunk_indexes=[]\n",
    "    for key, value in chunk_annotation.items():\n",
    "        if key==\"content\":\n",
    "            temp_prompt_str+=\"\\n\"+\", \".join(value)\n",
    "        else:\n",
    "            chunk_indexes=value\n",
    "            temp_prompt_str+=key\n",
    "    topic_prompts.append(temp_prompt_str)\n",
    "    \n",
    "    # get transcript prompt from the indexes\n",
    "    transcript_chunks_str=\"\"\n",
    "    for chunk_index in chunk_indexes:\n",
    "        transcript_chunks_str+=summarized_chunks[chunk_index][\"transcript\"]+\"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trancript_model_data=await saf.grab_transcript_data(video_id)\n",
    "chunk_annotations_str=await saf.recap_zoomed_in_generator.annotate_zoom_chunks(summaized_chunks=trancript_model_data.summarized_chunks, recap=recap)\n",
    "chunk_annotations, topic_prompts, transcript_chunks_prompts=saf.recap_zoomed_in_generator.prepare_zoom_inputs(trancript_model_data.summarized_chunks, chunk_annotations_str)\n",
    "zooms=await saf.recap_zoomed_in_generator.prepare_zoom_inputs(topic_prompts, transcript_chunks_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset saf \n",
    "del sys.modules['destinyapp.api.ServerAiFunctions']\n",
    "import destinyapp.api.ServerAiFunctions as saf\n",
    "reload(saf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_response=await saf.recap_zoomed_in_generator.generate_zoom(transcript_chunks_str=transcript_chunks_str, topic_prompt_str=topic_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zoom_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_chunks_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zoom_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zoom_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all zoom gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset saf \n",
    "del sys.modules['destinyapp.api.ServerAiFunctions']\n",
    "import destinyapp.api.ServerAiFunctions as saf\n",
    "reload(saf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trancript_model_data=await saf.grab_transcript_data(video_id)\n",
    "chunk_annotations_str=await saf.recap_zoomed_in_generator.annotate_zoom_chunks(summaized_chunks=trancript_model_data.summarized_chunks, recap=recap)\n",
    "chunk_annotations, topic_prompts, transcript_chunks_prompts=saf.recap_zoomed_in_generator.prepare_zoom_inputs(trancript_model_data.summarized_chunks, chunk_annotations_str)\n",
    "zooms=await saf.recap_zoomed_in_generator.generate_all_zooms(topic_prompts, transcript_chunks_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zooms=await saf.recap_zoomed_in_generator.generate_all_zooms(topic_prompts, transcript_chunks_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_prompts[0])\n",
    "print(\"\\n\")\n",
    "print(zooms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformt_dict=await saf.recap_zoomed_in_generator.reformat_recap(recap, topic_prompts, zooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the recap to get the [{\"title\": list_title, \"list_items\": list_items}] for each title and subsequent list items\n",
    "# basically find the stuff between headed tags, that is the title, continue until you see a <ul> tag which is the start of the list items\n",
    "\n",
    "\n",
    "# title_pattern=re.compile(r'<h3>(.*?)</h3>')\n",
    "# list_item_pattern=re.compile(r'<li>(.*?)</li>')\n",
    "# titles=title_pattern.findall(recap)\n",
    "# list_items=list_item_pattern.findall(recap)\n",
    "    \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(recap, 'html.parser')\n",
    "\n",
    "# Find all the titles and list items\n",
    "data = []\n",
    "for title in soup.find_all('h3'):\n",
    "    list_items = []\n",
    "    ul_tag = title.find_next_sibling('ul')\n",
    "    if ul_tag:\n",
    "        for li in ul_tag.find_all('li'):\n",
    "            list_items.append(li.text)\n",
    "    data.append({\"title\": title.text, \"list_items\": list_items})\n",
    "\n",
    "for title in soup.find_all('h2'):\n",
    "    list_items = []\n",
    "    ul_tag = title.find_next_sibling('ul')\n",
    "    if ul_tag:\n",
    "        for li in ul_tag.find_all('li'):\n",
    "            list_items.append(li.text)\n",
    "    data.append({\"title\": title.text, \"list_items\": list_items})\n",
    "\n",
    "print(data)\n",
    "\n",
    "    # return the titles and list items\n",
    "    # return [{\"title\": title, \"list_items\": list_items} for title, list_items in zip(titles, list_items)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(chunk_annotation.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_data=[]\n",
    "\n",
    "for piece in data:\n",
    "    for i, chunk_annotation  in enumerate(chunk_annotations):\n",
    "        if list(chunk_annotation.keys())[0] in piece[\"title\"]:\n",
    "            html_list=\"<ul>\"\n",
    "            for list_item in piece[\"list_items\"]:\n",
    "                html_list+=f\"<li>{list_item}</li>\"\n",
    "            html_list+=\"</ul>\"\n",
    "            filled_data.append({\"title\":piece[\"title\"], \"content\":html_list})\n",
    "            filled_data[-1][\"zoom\"]=zooms[i]\n",
    "\n",
    "    if piece[\"title\"]=='Smaller Details and Brief Mentions':\n",
    "        for small_piece in piece[\"list_items\"]:\n",
    "            for i, chunk_annotation  in enumerate(chunk_annotations):\n",
    "                if list(chunk_annotation.keys())[0] in small_piece:\n",
    "                    filled_data.append({\"title\":small_piece})\n",
    "                    filled_data[-1][\"zoom\"]=zooms[i]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import html\n",
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(recap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_zooms=0\n",
    "for zoom in zooms:\n",
    "    len_zooms+=len(zoom)\n",
    "print(len_zooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab which data is associated with each title\n",
    "import re\n",
    "def extract_titles_and_list_items(recap):\n",
    "    title_pattern=re.compile(r'<h3>(.*?)</h3>')\n",
    "    list_item_pattern=re.compile(r'<li>(.*?)</li>')\n",
    "    titles=title_pattern.findall(recap)\n",
    "    list_items=list_item_pattern.findall(recap)\n",
    "    return titles, list_items\n",
    "\n",
    "titles, list_items=extract_titles_and_list_items(recap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hook 1m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat v1@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload\n",
    "del sys.modules['destinyapp.api.ServerAiFunctions']\n",
    "import destinyapp.api.ServerAiFunctions as saf\n",
    "reload(saf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_id=\"3kJr7ODrwNw\"\n",
    "\n",
    "import datetime\n",
    "from chat_downloader import ChatDownloader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_meta_data = ChatDownloader().get_chat('https://www.youtube.com/watch?v='+yt_id)\n",
    "\n",
    "# get the chat data\n",
    "all_chat_messages=[]\n",
    "for chat in video_meta_data.chat:\n",
    "    all_chat_messages.append(chat)\n",
    "\n",
    "# Simplify the chat data\n",
    "simplified_messsages=[]\n",
    "for message in all_chat_messages:\n",
    "    simplified_messsages.append({\"name\": message[\"author\"][\"name\"], \"message\": message[\"message\"], \"time\": message[\"time_text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages=\"\"\n",
    "for i, message in enumerate(simplified_messsages):\n",
    "    all_messages+=f\"{i+1}. {message['message']}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(saf.enc.encode(all_messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the transcript\n",
    "transcript_data=await saf.grab_transcript_data(yt_id)\n",
    "transcript=transcript_data.transcript\n",
    "start_end_char=transcript_data.summarized_chunks[0][\"char_start_finish_indexes\"]\n",
    "transcript=transcript[start_end_char[0]:start_end_char[1]]\n",
    "linked_transcript=transcript_data.linked_transcript\n",
    "\n",
    "# get the messages in the segment\n",
    "start_time=saf.get_time_at_char_count(start_end_char[0], linked_transcript)\n",
    "finish_time=saf.get_time_at_char_count(start_end_char[1], linked_transcript)\n",
    "segment_messages=saf.get_chats_in_start_end(simplified_messsages,  int(start_time), int(finish_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simplified_messsages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propose a split size and then get the indexes to split the chat into batches\n",
    "chat_processing_batches=[]\n",
    "set_batch_size=150\n",
    "if len(segment_messages)<set_batch_size:\n",
    "    batch_size=len(segment_messages)\n",
    "else:\n",
    "    batch_size=math.ceil(len(segment_messages)/round(len(segment_messages)//set_batch_size))\n",
    "# Make set of indexes to split the chat into batches\n",
    "batch_indexes=[i for i in range(0, len(segment_messages), batch_size)]\n",
    "for i, index in enumerate(batch_indexes):\n",
    "    if index==batch_indexes[-1]:\n",
    "        chat_processing_batches.append(segment_messages[index:])\n",
    "    else:\n",
    "        chat_processing_batches.append(segment_messages[index:batch_indexes[i+1]])\n",
    "# Create chat str batches\n",
    "chat_segment_str_batches=[]\n",
    "for i, batch in enumerate(chat_processing_batches):\n",
    "    print(\"Len batch\", len(batch))\n",
    "    chat_segment_str=\"\"\n",
    "    for j, message in enumerate(batch):\n",
    "        message_at_time=message[\"time\"]\n",
    "        message_content=message[\"message\"]\n",
    "        temp_str=f\"{j}: {message_content}\"\n",
    "        chat_segment_str+=temp_str+\"\\n\"\n",
    "    chat_segment_str_batches.append(chat_segment_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bot_analysis=await saf.analyze_chat(transcript_data.summarized_chunks[0][\"summary\"], chat_segment_str_batches[0])\n",
    "bot_analysis=await saf.analyze_chat(transcript_data.summarized_chunks[0][\"summary\"], chat_segment_str_batches[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bot_analysis.split(\"\\n\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total cost\n",
    "0.0025700000000000002*(len(simplified_messsages)/150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_annotations={}\n",
    "for line in bot_analysis.split(\"\\n\"):\n",
    "    number=line.split(\":\")[0]\n",
    "    if number.isdigit():\n",
    "        full_reason=line.split(\":\")[1].strip()\n",
    "        serious=full_reason.startswith(\"yes\")\n",
    "        chat_annotations[number]={\"full\": full_reason, \"serious\": serious}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_annotations['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats={}\n",
    "stats[\"serious\"]=0\n",
    "for chat_annotation in chat_annotations:\n",
    "    if chat_annotations[chat_annotation][\"serious\"]:\n",
    "        stats[\"serious\"]+=1\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_messsages[587]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chat_annotation in enumerate(chat_annotations):\n",
    "    if chat_annotations[chat_annotation][\"serious\"]:\n",
    "        y_n=\"y\"\n",
    "    else:\n",
    "        y_n=\"n\"\n",
    "    print(y_n, chat_segment_str_batches[3].split(\"\\n\")[i])#, \"                              ANALYSIS\", chat_annotations[chat_annotation][\"full\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_messsages[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_segment_str_batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process all chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload\n",
    "del sys.modules['destinyapp.api.ServerAiFunctions']\n",
    "import destinyapp.api.ServerAiFunctions as saf\n",
    "reload(saf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_id=\"3kJr7ODrwNw\"\n",
    "\n",
    "import datetime\n",
    "from chat_downloader import ChatDownloader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET TRANSCRIPT\n",
    "transcript_data=await saf.grab_transcript_data(yt_id)\n",
    "transcript=transcript_data.transcript\n",
    "linked_transcript=transcript_data.linked_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET BASE CHAT DATA\n",
    "video_meta_data = ChatDownloader().get_chat('https://www.youtube.com/watch?v='+yt_id)\n",
    "\n",
    "# get the chat data\n",
    "all_chat_messages=[]\n",
    "for chat in video_meta_data.chat:\n",
    "    all_chat_messages.append(chat)\n",
    "\n",
    "# Simplify the chat data\n",
    "simplified_messsages=[]\n",
    "for message in all_chat_messages:\n",
    "    simplified_messsages.append({\"name\": message[\"author\"][\"name\"], \"message\": message[\"message\"], \"time\": message[\"time_text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tasks and reference data\n",
    "tasks=[]\n",
    "input_data=[]\n",
    "\n",
    "for summarized_chunk in transcript_data.summarized_chunks:\n",
    "    # get the transcript\n",
    "    start_end_char=summarized_chunk[\"char_start_finsih_indexes\"]\n",
    "    transcript=transcript_data.transcript[start_end_char[0]:start_end_char[1]]\n",
    "\n",
    "    # get the messages in the segment\n",
    "    start_time=saf.get_time_at_char_count(start_end_char[0], linked_transcript)\n",
    "    finish_time=saf.get_time_at_char_count(start_end_char[1], linked_transcript)\n",
    "    segment_messages=saf.get_chats_in_start_end(simplified_messsages,  int(start_time), int(finish_time))\n",
    "\n",
    "    # # CREATE CHAT BATCHES\n",
    "    # propose a split size and then get the indexes to split the chat into batches\n",
    "    chat_processing_batches=[]\n",
    "    set_batch_size=150\n",
    "    if len(segment_messages)<set_batch_size:\n",
    "        batch_size=len(segment_messages)\n",
    "    else:\n",
    "        batch_size=math.ceil(len(segment_messages)/round(len(segment_messages)//set_batch_size))\n",
    "    # Make set of indexes to split the chat into batches\n",
    "    batch_indexes=[i for i in range(0, len(segment_messages), batch_size)]\n",
    "    for i, index in enumerate(batch_indexes):\n",
    "        if index==batch_indexes[-1]:\n",
    "            chat_processing_batches.append(segment_messages[index:])\n",
    "        else:\n",
    "            chat_processing_batches.append(segment_messages[index:batch_indexes[i+1]])\n",
    "    # Create chat str batches\n",
    "    chat_segment_str_batches=[]\n",
    "    for i, batch in enumerate(chat_processing_batches):\n",
    "        print(\"Len batch\", len(batch))\n",
    "        chat_segment_str=\"\"\n",
    "        for j, message in enumerate(batch):\n",
    "            message_at_time=message[\"time\"]\n",
    "            message_content=message[\"message\"]\n",
    "            temp_str=f\"{j}: {message_content}\"\n",
    "            chat_segment_str+=temp_str+\"\\n\"\n",
    "        chat_segment_str_batches.append(chat_segment_str)\n",
    "\n",
    "    for i, chat_batch in enumerate(chat_segment_str_batches):\n",
    "        input_data.append({\"transcript\": transcript, \"chat_batch\": chat_batch, \"segment_messages\": chat_processing_batches[i]})\n",
    "        tasks.append(saf.analyze_chat(transcript, chat_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN TASKS\n",
    "# split the tasks into batches of about 20 but make them as even as possible with math\n",
    "split_up_tasks=[]\n",
    "proposed_batch_size=20\n",
    "batch_size=math.ceil(len(tasks)/round(len(tasks)//proposed_batch_size))\n",
    "batch_indexes=[i for i in range(0, len(tasks), batch_size)]\n",
    "for i, index in enumerate(batch_indexes):\n",
    "    if index==batch_indexes[-1]:\n",
    "        split_up_tasks.append(tasks[index:])\n",
    "    else:\n",
    "        split_up_tasks.append(tasks[index:batch_indexes[i+1]])\n",
    "\n",
    "# run the tasks\n",
    "all_chat_analysis=[]\n",
    "for tasks_batch in split_up_tasks:\n",
    "    all_chat_analysis+=await asyncio.gather(*tasks_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize Analysis responses\n",
    "all_chat_annotations=[]\n",
    "error_count=0\n",
    "for i, chat_analysis in enumerate(all_chat_analysis):\n",
    "    chat_annotations={}\n",
    "    for c, line in enumerate(chat_analysis.split(\"\\n\")):\n",
    "        number=line.split(\":\")[0]\n",
    "        if number.isdigit():\n",
    "            full_reason=line.split(\":\")[1].strip()\n",
    "            serious=full_reason.startswith(\"yes\")\n",
    "            try:\n",
    "                chat_annotations[number]={\"full\": full_reason, \"serious\": serious, \"chat_data\": input_data[i][\"segment_messages\"][int(number)], \"transcript\":input_data[i][\"transcript\"]}\n",
    "            except:\n",
    "                error_count+=1\n",
    "    all_chat_annotations.append(chat_annotations)\n",
    "print(error_count)\n",
    "\n",
    "ordered_chat_annotations={}\n",
    "message_count=0\n",
    "stats={}\n",
    "stats[\"serious\"]=0\n",
    "for chat_annotations in all_chat_annotations:\n",
    "    for chat_annotation in list(chat_annotations.values()):\n",
    "        ordered_chat_annotations[simplified_messsages.index(chat_annotation[\"chat_data\"])]=chat_annotation\n",
    "        if chat_annotation[\"serious\"]:\n",
    "            stats[\"serious\"]+=1\n",
    "        message_count+=1\n",
    "    \n",
    "print(message_count)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chat_annotation(chat_annotation):\n",
    "    print(chat_annotation[\"full\"], chat_annotation[\"serious\"], chat_annotation[\"chat_data\"])\n",
    "chat_annotations=all_chat_annotations[0]\n",
    "print_chat_annotation(chat_annotations['1'])\n",
    "chat_annotations=all_chat_annotations[1]\n",
    "print_chat_annotation(chat_annotations['1'])\n",
    "chat_annotations=all_chat_annotations[2]\n",
    "print_chat_annotation(chat_annotations['1'])\n",
    "chat_annotations=all_chat_annotations[3]\n",
    "print_chat_annotation(chat_annotations['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_chat_annotations=list(ordered_chat_annotations.values())\n",
    "stats={}\n",
    "stats[\"serious\"]=0\n",
    "serious_messages=[]\n",
    "\n",
    "for key, chat_annotation in ordered_chat_annotations.items():\n",
    "    if chat_annotation[\"serious\"]:\n",
    "        stats[\"serious\"]+=1\n",
    "        serious_messages.append({key: chat_annotation})\n",
    "    \n",
    "print(stats)\n",
    "print(len(list_chat_annotations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serious_message in serious_messages:\n",
    "    for key, value in serious_message.items():\n",
    "        print(simplified_messsages[key], value[\"chat_data\"], value[\"full\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serious_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_c=0\n",
    "for key, value in serious_message.items():\n",
    "    # print(key, value)\n",
    "    item_c+=1\n",
    "print(item_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_segment_serious_messages=[]\n",
    "temp_segment=[]\n",
    "previous_transcript=None\n",
    "for serious_message in serious_messages:\n",
    "    for key, value in serious_message.items():\n",
    "        if (value[\"transcript\"]!=previous_transcript) and (previous_transcript!=None):\n",
    "            same_segment_serious_messages.append(temp_segment)\n",
    "            temp_segment=[]\n",
    "        else:\n",
    "            temp_segment.append(value)\n",
    "\n",
    "        previous_transcript=value[\"transcript\"]\n",
    "\n",
    "if temp_segment!=[]:\n",
    "    same_segment_serious_messages.append(temp_segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment_serious_messages in same_segment_serious_messages:\n",
    "    print(len(segment_serious_messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_segment_serious_messages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDO RECAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "import discord\n",
    "import html2text\n",
    "import traceback\n",
    "all_transcript_data=await saf.get_all_data()\n",
    "keys=views.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "video_id=\"5raed64fL0Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id=\"I3FuM7myMrI\"\n",
    "video_id=\"N2lCEccQNvY\"\n",
    "video_id=\"anDEECKsCfc\"\n",
    "video_id=\"ys64pMzpDUs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.youtube.com/watch?v='+video_id\n",
    "yt = YouTube(url)\n",
    "\n",
    "\n",
    "raw_date=yt.publish_date.__str__()\n",
    "date_obj=datetime.datetime.strptime(raw_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "date_str=date_obj.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video\n",
    "yt = YouTube(url)\n",
    "\n",
    "# Fill attributes\n",
    "try:\n",
    "    for attr in dir(yt):\n",
    "        value=getattr(yt, attr)\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Get Live status\n",
    "try:\n",
    "    vid_info=getattr(yt, \"_vid_info\")\n",
    "    live_bool=vid_info[\"videoDetails\"][\"isLive\"]\n",
    "except Exception as e:\n",
    "    live_bool=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print every attribute of the video\n",
    "for attr in dir(yt):\n",
    "    try:\n",
    "        print(f\"yt.{attr}\")# = {getattr(yt, attr)}\")\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_dl\n",
    "ydl_opts = {}\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.extract_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp as youtube_dl\n",
    "\n",
    "ydl_opts = {}\n",
    "full_title=\"\"\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    info_dict = ydl.extract_info(f\"https://www.youtube.com/watch?v={video_id}\", download=False)\n",
    "    info_dict_g=info_dict\n",
    "    upload_date = info_dict['upload_date']\n",
    "    upload_date\n",
    "    date_obj=datetime.datetime.strptime(upload_date, \"%Y%m%d\")\n",
    "    date_str=date_obj.strftime(\"%m/%d/%Y\")\n",
    "    title=info_dict[\"title\"]\n",
    "    full_title=title+\"\\nStream Date: \"+date_str\n",
    "\n",
    "print(full_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some video metadata\n",
    "async def get_video_metadata(video_id):\n",
    "    ydl_opts = {}\n",
    "    full_title=\"\"\n",
    "    try:\n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            info_dict = ydl.extract_info(f\"https://www.youtube.com/watch?v={video_id}\", download=False)\n",
    "            info_dict_g=info_dict\n",
    "            upload_date = info_dict['upload_date']\n",
    "            upload_date\n",
    "            date_obj=datetime.datetime.strptime(upload_date, \"%Y%m%d\")\n",
    "            date_str=date_obj.strftime(\"%m/%d/%Y\")\n",
    "            title=info_dict[\"title\"]\n",
    "            full_title=title+\"\\nStream Date~ \"+date_str\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return full_title\n",
    "\n",
    "await get_video_metadata(\"ZBcQEnCDgDg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_date\n",
    "date_obj=datetime.datetime.strptime(upload_date, \"%Y%m%d\")\n",
    "date_str=date_obj.strftime(\"%m/%d/%Y\")\n",
    "title=info_dict[\"title\"]\n",
    "full_title=yt.title+\"\\nStream Date: \"+date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt._vid_info[\"videoDetails\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def redo_recap(transcript_model_datas, vector_embeedding_bool=True, summary_segments_bool=True, meta_summary_bool=True, video_metadata_bool=True):\n",
    "\n",
    "    discord_recaps_to_send=[]\n",
    "    for transcript_model_data in transcript_model_datas:\n",
    "        yt_id=transcript_model_data.video_id\n",
    "        print(\"REDOING RECAP FOR: \", yt_id)\n",
    "        raw_transcript_data=transcript_model_data.raw_transcript_data\n",
    "        save_raw_transcript_data={\"raw_transcript_data\": raw_transcript_data}\n",
    "\n",
    "        # Process to make regular and linked_transcript\n",
    "        save_processed_transcripts=await saf.process_raw_transcript(raw_transcript_data, yt_id)\n",
    "        transcript=save_processed_transcripts[\"transcript\"]\n",
    "        linked_transcript=save_processed_transcripts[\"linked_transcript\"]\n",
    "        print(\"Transcript finished\")#, transcript)\n",
    "\n",
    "        # Make vector db\n",
    "        if vector_embeedding_bool:\n",
    "            vectordb_and_textchunks=await saf.assembly_generate_vectordb_and_chunks(yt_id, save_processed_transcripts[\"transcript\"])\n",
    "            text_chunks=vectordb_and_textchunks[\"text_chunks\"]\n",
    "            print(\"Text Chunks Finished\")# \", text_chunks)\n",
    "\n",
    "        # Generate summarized segments\n",
    "        if summary_segments_bool:\n",
    "            model_responses=await saf.generate_summarized_segments(save_processed_transcripts[\"transcript\"])#,[], 10)\n",
    "            print(\"Summarized Chunks Finished\")#, model_responses)\n",
    "\n",
    "        # Make meta summary\n",
    "        if meta_summary_bool:\n",
    "            # meta_summary=await saf.generate_meta_summary(model_responses)\n",
    "            meta_summary=await saf.meta_summary_generator.generate_meta_summary(model_responses)\n",
    "            print(\"Meta Summary Finished: \", meta_summary)\n",
    "\n",
    "            # add the hook to the meta summary\n",
    "            recap_hook=await saf.generate_recap_hook(meta_summary)\n",
    "            meta_summary=recap_hook+\"\\n\"+meta_summary+\"\\n\\nDISCLAIMER: This is all AI generated and there are frequent errors.\"\n",
    "\n",
    "\n",
    "        # get some video metadata\n",
    "        if video_metadata_bool:\n",
    "            async def get_video_metadata(video_id):\n",
    "                url = 'https://www.youtube.com/watch?v='+video_id\n",
    "                yt = YouTube(url)\n",
    "\n",
    "                raw_date=yt.publish_date.__str__()\n",
    "                date_obj=datetime.datetime.strptime(raw_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "                date_str=date_obj.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "                full_title=yt.title+\"\\nStream Date: \"+date_str\n",
    "\n",
    "                return full_title\n",
    "            full_title=await get_video_metadata(yt_id)\n",
    "\n",
    "        # Save everything\n",
    "        await saf.save_data(yt_id, save_processed_transcripts)\n",
    "        if vector_embeedding_bool:\n",
    "            await saf.save_data(yt_id, {\"text_chunks\":vectordb_and_textchunks[\"text_chunks\"]})\n",
    "        if summary_segments_bool:\n",
    "            await saf.save_data(yt_id, {\"summarized_chunks\":model_responses})\n",
    "        if meta_summary_bool:\n",
    "            await saf.save_data(yt_id, {\"meta\":meta_summary})\n",
    "        if video_metadata_bool:\n",
    "            await saf.save_data(yt_id, {\"video_characteristics\":{\"title\":full_title}})\n",
    "\n",
    "        \n",
    "        # Setup discord to send message\n",
    "        new_transcript_model_data=await saf.grab_transcript_data(yt_id)\n",
    "        if new_transcript_model_data.video_characteristics.get(\"title\", None)!=None:\n",
    "            full_title=new_transcript_model_data.video_characteristics[\"title\"]\n",
    "        else:\n",
    "            full_title=None\n",
    "        discord_recaps_to_send.append({\"meta\":new_transcript_model_data.meta,\"yt_id\":yt_id, \"title\":full_title})\n",
    "\n",
    "    \n",
    "    # Using discord_recaps_to_send to send recaps to discord\n",
    "    async def send_discord_recaps():\n",
    "        class MessageSendingClient(discord.Client):\n",
    "            async def on_ready(self):\n",
    "                async def send_recap(recap):\n",
    "                    # Send discord message header\n",
    "                    destinyrecaps_url=\"https://destinyrecaps.com\"+\"/details?video_id=\"+recap[\"yt_id\"]\n",
    "                    destinyrecaps_msg=\"Full transcript and embedding search at \"+destinyrecaps_url\n",
    "                    if recap.get(\"title\",None)!=None:\n",
    "                        youtube_msg=recap[\"title\"]+\": \"+\"https://www.youtube.com/watch?v=\"+recap[\"yt_id\"]\n",
    "                    else:\n",
    "                        youtube_msg=\"https://www.youtube.com/watch?v=\"+recap[\"yt_id\"]\n",
    "\n",
    "                    header_message=f\"{youtube_msg}\\n{destinyrecaps_msg}\"\n",
    "                    await channel.send(header_message)\n",
    "\n",
    "                    # initialize variables for recap message\n",
    "                    tag_message=\"@everyone \\n\"\n",
    "                    message_str=tag_message+html2text.html2text(recap[\"meta\"])\n",
    "                    start_index=0\n",
    "                    recap_chunks={}\n",
    "                    recap_chunks[\"start_finish\"]=[0]\n",
    "                    recap_chunks[\"segments\"]=[]\n",
    "                    increment_size=1500\n",
    "\n",
    "                    # increment for the number of segments needed\n",
    "                    for i in range((len(message_str)//increment_size)+1):\n",
    "                        \n",
    "                        # find the the reasonable end of the segment\n",
    "                        finish_index=start_index+increment_size\n",
    "                        if finish_index>=len(message_str):\n",
    "                            finish_index=None\n",
    "                        else:\n",
    "                            while message_str[finish_index]!=\"\\n\":\n",
    "                                finish_index+=1\n",
    "                                if (finish_index-start_index)>2100:\n",
    "                                    print(\"Didn't find a newline\")\n",
    "                                    break\n",
    "                                if finish_index>=len(message_str):\n",
    "                                    finish_index=None\n",
    "                                    break\n",
    "                        \n",
    "                        # append the segments to the list\n",
    "                        recap_chunks[\"segments\"].append(message_str[start_index:finish_index])\n",
    "                        start_index=finish_index\n",
    "                        recap_chunks[\"start_finish\"].append(start_index)\n",
    "\n",
    "                        if finish_index==None:\n",
    "                            break\n",
    "\n",
    "                    print(\"Sending c-hunks: \",len(recap_chunks[\"segments\"]))\n",
    "                    for recap_chunk in recap_chunks[\"segments\"]:\n",
    "                        await channel.send(recap_chunk)\n",
    "\n",
    "\n",
    "                print(f'Discord logged in as {self.user}')\n",
    "                channels=self.get_all_channels()\n",
    "                for channel in channels:\n",
    "                    print(channel.name)\n",
    "                    if channel.name==\"recaps\":\n",
    "                        if channel:\n",
    "                            for recap in discord_recaps_to_send:\n",
    "                                print(\"Sending Recap\")\n",
    "                                try:\n",
    "                                    await send_recap(recap)\n",
    "                                except Exception as e:\n",
    "                                    # print as much as possible\n",
    "                                    print(\"ERROR: \",e)\n",
    "                                    print(traceback.format_exc())\n",
    "\n",
    "                await self.close()\n",
    "                print(\"Send and client closed\")\n",
    "\n",
    "        intents = discord.Intents.default()\n",
    "        intents.messages = True \n",
    "        client = MessageSendingClient(intents=intents)\n",
    "\n",
    "        await client.start(keys[\"discord\"])\n",
    "\n",
    "    # start the discord recaps sending\n",
    "    try:\n",
    "        await send_discord_recaps()\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: \",e)\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def redo_recap_controller(item_count=3):\n",
    "    all_transcript_data = await saf.get_all_data()\n",
    "    if item_count>len(all_transcript_data):\n",
    "        item_count=len(all_transcript_data)\n",
    "    print(f\"REDOING RECAPS FOR {item_count} ITEMS\")\n",
    "    for i, transcript_model_data in enumerate(all_transcript_data[::-1][:item_count]):\n",
    "        print(i, transcript_model_data.video_id, transcript_model_data.video_characteristics)\n",
    "\n",
    "    for i, transcript_model_data in enumerate(all_transcript_data[::-1][:item_count]):\n",
    "        print(\"Redoing:\",i, transcript_model_data.video_id, transcript_model_data.video_characteristics)\n",
    "        await redo_recap([transcript_model_data], transcript_model_data.video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await redo_recap_controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, transcript_model_data in enumerate(all_transcript_data[::-1]):\n",
    "    if i==3:\n",
    "        print(i, transcript_model_data.video_id, transcript_model_data.video_characteristics)\n",
    "        await redo_recap([transcript_model_data], transcript_model_data.video_id)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_d_i in all_transcript_data:\n",
    "    print(t_d_i.video_characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_transcript_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from the database\n",
    "video_id=\"hAf0iOS-2V4\"\n",
    "meta_data=await sync_to_async(TranscriptData.objects.get)(video_id=video_id)\n",
    "\n",
    "# Get the raw_transcript_data\n",
    "raw_transcript_data = meta_data.raw_transcript_data\n",
    "# Get the size in bytes of raw_transcript_data\n",
    "raw_transcript_size = sys.getsizeof(raw_transcript_data)\n",
    "\n",
    "# Get the linked_transcript\n",
    "linked_transcript = meta_data.linked_transcript\n",
    "# Get the size in bytes of linked_transcript\n",
    "linked_transcript_size = sys.getsizeof(linked_transcript)\n",
    "\n",
    "print(f\"Size of raw_transcript_data: {raw_transcript_size} bytes\")\n",
    "print(f\"Size of linked_transcript: {linked_transcript_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_transcript_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meta_data.linked_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(str(raw_transcript_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_transcript_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
