{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DestinyFolder\\DestinyRecaps\\DestinyRecapsApi\\serverproject\n"
     ]
    }
   ],
   "source": [
    "# DIRECTORY SET\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "base_dir=Path(os.getcwd()).parent\n",
    "# os.chdir(os.path.join(base_dir, 'serverproject'))\n",
    "os.chdir(base_dir)\n",
    "print(os.getcwd())\n",
    "\n",
    "# Load dotenv\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# DJANGO SETUP\n",
    "import django\n",
    "sys.path.append(os.path.abspath(''))\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"serverproject.settings\")\n",
    "django.setup()\n",
    "\n",
    "# Import async modules\n",
    "import asyncio\n",
    "from asgiref.sync import sync_to_async\n",
    "\n",
    "# Import display modules\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Import other modules\n",
    "import faiss\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from destinyapp.models import StreamRecapData, FastRecapData\n",
    "\n",
    "from core import services\n",
    "from core import utils\n",
    "from core import controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"text-embedding-3-large\"\n",
    "\n",
    "async def fetch_embedding(chunk):\n",
    "    # Simulate an async call to the embeddings API\n",
    "    #return await asyncio.to_thread(openai_client.embeddings.create, input=chunk, model=model)\n",
    "    model=\"text-embedding-3-large\"        \n",
    "\n",
    "    fails=0\n",
    "    while fails<5:\n",
    "        try:\n",
    "            return await utils.async_openai_client.embeddings.create(input=chunk, model=model)\n",
    "        except Exception as e:\n",
    "            fails+=1\n",
    "            print(\"Emedding Fail Retrying:\",e)\n",
    "            await asyncio.sleep(10+(fails*2))\n",
    "    return None\n",
    "\n",
    "async def generate_embeddings_async(text_chunks, model):\n",
    "\n",
    "    responses = await asyncio.gather(*(fetch_embedding(chunk) for chunk in text_chunks))\n",
    "    embeddings = [response.data[0].embedding for response in responses]\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding.data[0].embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DestinyFolder\\DestinyRecaps\\DestinyRecapsApi\\serverproject\n",
      "d:\\DestinyFolder\\DestinyRecaps\\DestinyRecapsApi\\serverproject\\search_dev_notebooks\\working\\indexes\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "indexes_path=os.path.join(os.getcwd(),\"search_dev_notebooks\",\"working\",\"indexes\")\n",
    "print(indexes_path)\n",
    "print(os.path.isdir(indexes_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_embed=\"This is a test\"\n",
    "\n",
    "embedding=await fetch_embedding(text_to_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=len(embedding.data[0].embedding)\n",
    "\n",
    "embedding_data=np.array(embedding.data[0].embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a faiss index\n",
    "vector_db=faiss.IndexFlatL2(embedding_size)\n",
    "vector_db.add(np.array([embedding_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# larger index creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=await fetch_embedding(\"get embedding size\")\n",
    "embedding_size=len(embedding.data[0].embedding)\n",
    "vector_db=faiss.IndexFlatL2(embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigger faiss index\n",
    "texts=[\"This is a test\", \"The quick brown fox jumps over the lazy dog\", \"I love eating pizza on rainy days\", \"Machine learning is fascinating\", \"The sunset painted the sky in brilliant colors\", \"She danced through fields of wildflowers\", \"The old library held countless untold stories\", \"Coffee is essential for Monday mornings\", \"Waves crashed against the rocky shore\", \"Dragons soared through cloudy skies\"]\n",
    "embeddings=await generate_embeddings_async(texts, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.add(np.array(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the index to a file\n",
    "faiss.write_index(vector_db, os.path.join(indexes_path, \"test_index.faiss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the index from a file\n",
    "vector_db=faiss.read_index(os.path.join(indexes_path, \"test_index.faiss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the index\n",
    "query_text=\"This is a test\"\n",
    "query_embedding=await fetch_embedding(query_text)\n",
    "query_embedding_np = np.array(query_embedding.data[0].embedding).astype('float32').reshape(1, -1)\n",
    "k_size=5\n",
    "k=vector_db.ntotal\n",
    "if k>k_size:\n",
    "    k=k_size\n",
    "\n",
    "D, I = vector_db.search(query_embedding_np, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[2.1857386e-06 1.1849377e+00 1.6127548e+00 1.6167562e+00 1.6696446e+00]]\n",
      "I: [[0 1 3 7 8]]\n"
     ]
    }
   ],
   "source": [
    "print(\"D:\",D)\n",
    "print(\"I:\",I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = np.asarray(query_embedding.data[0].embedding)\n",
    "embedding_matrix = np.asarray(embeddings)\n",
    "\n",
    "# Reshape query to 1D if needed\n",
    "if len(query.shape) > 1:\n",
    "    query = query.reshape(-1)\n",
    "    \n",
    "# Calculate euclidean distances using broadcasting\n",
    "# This is more efficient than looping\n",
    "distances = np.sqrt(np.sum((embedding_matrix - query) ** 2, axis=1))\n",
    "\n",
    "# Get indices of k smallest distances\n",
    "nearest_indices = np.argsort(distances)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00147842, 1.08854847, 1.34954656, 1.26994282, 1.33518695,\n",
       "       1.34190124, 1.35120326, 1.27151728, 1.29214726, 1.29558511])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nearest_indices: [0 1 3 7 8]\n"
     ]
    }
   ],
   "source": [
    "print(\"nearest_indices:\",nearest_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finish of larger index creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the index to a file\n",
    "faiss.write_index(vector_db, os.path.join(indexes_path, \"test_index.faiss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the index from a file\n",
    "vector_db=faiss.read_index(os.path.join(indexes_path, \"test_index.faiss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the index\n",
    "query_text=\"This is a test\"\n",
    "query_embedding=await fetch_embedding(query_text)\n",
    "query_embedding_np = np.array(query_embedding.data[0].embedding).astype('float32').reshape(1, -1)\n",
    "k_size=5\n",
    "k=vector_db.ntotal\n",
    "if k>k_size:\n",
    "    k=k_size\n",
    "\n",
    "D, I = vector_db.search(query_embedding_np, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[2.1857386e-06]]\n",
      "I: [[0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"D:\",D)\n",
    "print(\"I:\",I)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
