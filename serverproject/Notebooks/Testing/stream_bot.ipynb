{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DestinyFolder\\DestinyRecaps\\DestinyRecapsApi\\serverproject\n"
     ]
    }
   ],
   "source": [
    "# DIRECTORY SET\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "base_dir=Path(os.getcwd()).parent.parent\n",
    "os.chdir(base_dir)\n",
    "print(os.getcwd())\n",
    "\n",
    "# ENVIRONMENT VARIABLES\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# DJANGO SETUP\n",
    "import django\n",
    "sys.path.append(os.path.abspath(''))\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"serverproject.settings\")\n",
    "django.setup()\n",
    "\n",
    "# Import async modules\n",
    "import asyncio\n",
    "from asgiref.sync import sync_to_async\n",
    "\n",
    "# Import display modules\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Import other modules\n",
    "import faiss\n",
    "\n",
    "# import reloading\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from destinyapp.models import StreamRecapData\n",
    "\n",
    "from destinyapp.customlibrary import services\n",
    "from destinyapp.customlibrary import utils\n",
    "from destinyapp.customlibrary import controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id=\"OqVH_MTBQ6k\"\n",
    "stream_recap_data=await utils.get_recap_data(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db, text_chunks = await services.VectorDbAndTextChunksGenerator.generate_basic_vectordb_and_chunks(video_id, stream_recap_data.transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt=\"Can you explain what the federalist papers are?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results=await services.search(video_id, user_prompt, k_size=10)#, vector_db, text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the overlapping text segments\n",
    "start_stops=[[search_results[\"all_indexes\"][0], search_results[\"all_indexes\"][0]+1000]]\n",
    "for index in search_results[\"all_indexes\"][1:]:\n",
    "    diff_to_last=index-start_stops[-1][1]\n",
    "    if (diff_to_last <= 500) and (diff_to_last >= -500):\n",
    "        if diff_to_last > 0:\n",
    "            start_stops[-1][1]=start_stops[-1][1]+diff_to_last\n",
    "        else:\n",
    "            start_stops[-1][0]=start_stops[-1][0]+diff_to_last\n",
    "        continue\n",
    "\n",
    "    diff_to_last=index-start_stops[-1][0]\n",
    "    if (diff_to_last <= 500) and (diff_to_last >= -500):\n",
    "        if diff_to_last > 0:\n",
    "            start_stops[-1][1]=start_stops[-1][1]+diff_to_last\n",
    "        else:\n",
    "            start_stops[-1][0]=start_stops[-1][0]+diff_to_last\n",
    "        continue\n",
    "    \n",
    "    start_stops.append([index, index+1000])\n",
    "\n",
    "# produce the segments as a string\n",
    "rag_context_str=\"\"\n",
    "for i, ss in enumerate(start_stops):\n",
    "    rag_context_str+=f\"Chunk {i}: \"+stream_recap_data.transcript[ss[0]:ss[1]]+\"\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt=\"\"\"You are a stream bot. You engauge with the user with respect to a past livestream.\n",
    "\n",
    "You will be given context from the stream the user is talking about by method of RAG. Do your best to accuracy answer the user's question or engage intelligently given the context of the stream. \n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the recap for the stream you are to be knowledgeable about:\n",
    "{stream_recap}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the RAG context raw from the transcript that is potentially relevant to what the user is saying:\n",
    "{rag_context}\n",
    "\n",
    "--------------------------------------------\n",
    "Always try to be concise in what you are saying and talk about things in the direct context of the stream or stream recap.\n",
    "\n",
    "\"\"\".format(stream_recap=stream_recap_data.recap, rag_context=rag_context_str)\n",
    "\n",
    "full_prompt=[{\"role\":\"system\", \"content\":system_prompt}, {\"role\":\"user\", \"content\":user_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=await utils.async_response_handler(full_prompt, utils.ModelNameEnum.gpt_4o_mini)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamBot:\n",
    "    chat_history=[]\n",
    "\n",
    "    @classmethod\n",
    "    async def answer_user(self, video_id, user_prompt, test=None):\n",
    "\n",
    "        stream_recap_data=await utils.get_recap_data(video_id)\n",
    "\n",
    "        search_results=await services.search(video_id, user_prompt, k_size=10)#, vector_db, text_chunks \n",
    "\n",
    "        # merge the overlapping text segments\n",
    "        start_stops=[[search_results[\"all_indexes\"][0], search_results[\"all_indexes\"][0]+1000]]\n",
    "        for index in search_results[\"all_indexes\"][1:]:\n",
    "            diff_to_last=index-start_stops[-1][1]\n",
    "            if (diff_to_last <= 500) and (diff_to_last >= -500):\n",
    "                if diff_to_last > 0:\n",
    "                    start_stops[-1][1]=start_stops[-1][1]+diff_to_last\n",
    "                else:\n",
    "                    start_stops[-1][0]=start_stops[-1][0]+diff_to_last\n",
    "                continue\n",
    "\n",
    "            diff_to_last=index-start_stops[-1][0]\n",
    "            if (diff_to_last <= 500) and (diff_to_last >= -500):\n",
    "                if diff_to_last > 0:\n",
    "                    start_stops[-1][1]=start_stops[-1][1]+diff_to_last\n",
    "                else:\n",
    "                    start_stops[-1][0]=start_stops[-1][0]+diff_to_last\n",
    "                continue\n",
    "            \n",
    "            start_stops.append([index, index+1000])\n",
    "\n",
    "        # produce the segments as a string\n",
    "        rag_context_str=\"\"\n",
    "        for i, ss in enumerate(start_stops):\n",
    "            rag_context_str+=f\"Chunk {i}: \"+stream_recap_data.transcript[ss[0]:ss[1]]+\"\\n\\n\"\n",
    "        \n",
    "\n",
    "        # Compile prompt\n",
    "        system_prompt=\"\"\"You are a stream bot. You engauge with the user with respect to a past livestream.\n",
    "\n",
    "        You will be given context from the stream the user is talking about by method of RAG. Do your best to accuracy answer the user's question or engage intelligently given the context of the stream. \n",
    "\n",
    "        --------------------------------------------\n",
    "\n",
    "        Here is the recap for the stream you are to be knowledgeable about:\n",
    "        {stream_recap}\n",
    "\n",
    "        --------------------------------------------\n",
    "\n",
    "        Here is the RAG context raw from the transcript that is potentially relevant to what the user is saying:\n",
    "        {rag_context}\n",
    "\n",
    "        --------------------------------------------\n",
    "        Always try to be concise in what you are saying and talk about things in the direct context of the stream or stream recap.\n",
    "\n",
    "        \"\"\".format(stream_recap=stream_recap_data.recap, rag_context=rag_context_str)\n",
    "\n",
    "        if self.chat_history!=[]:\n",
    "            self.chat_history=[{\"role\":\"system\", \"content\":system_prompt}, {\"role\":\"user\", \"content\":user_prompt}]\n",
    "        else:\n",
    "            self.chat_history[0][\"content\"]=system_prompt\n",
    "            self.chat_history.append({\"role\":\"user\", \"content\":user_prompt})\n",
    "\n",
    "        response=await utils.async_response_handler(full_prompt, utils.ModelNameEnum.gpt_4o_mini)\n",
    "        self.chat_history.append({\"role\":\"system\", \"content\":response})\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_bot=services.StreamBot()\n",
    "video_id=\"OqVH_MTBQ6k\"\n",
    "user_prompt=\"Can you explain what the federalist papers are?\"\n",
    "stream_bot.chat_history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt=input(\"User Prompt: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  0.0012564\n",
      "In the stream, the importance of judicial independence was emphasized as a crucial safeguard for the Constitution and individual rights. The discussion highlighted that an independent judiciary is essential to protect against political pressures and the influence of the legislative and executive branches. It was argued that lifetime appointments for judges help ensure this independence, allowing them to make decisions based solely on the law and the Constitution rather than on popular opinion or political considerations. The participants noted that a strong and independent judiciary serves as a check on government power, which is vital for maintaining the rule of law and protecting minority rights against potential abuses by the majority.\n"
     ]
    }
   ],
   "source": [
    "response=await stream_bot.answer_user(video_id, user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://127.0.0.1:8000/api/chatbot_response?pin=194&video_id=OqVH_MTBQ6k'\n",
    "# set the chat_history in the body\n",
    "\n",
    "request_body={\"chat_history\":[\"test\"]}\n",
    "\n",
    "response=requests.post(url, json=request_body)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'chatbot_response'}\n"
     ]
    }
   ],
   "source": [
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 2)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32md:\\DestinyFolder\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[15], line 3\u001b[0m\n    response_dict=ast.literal_eval(response.text)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mC:\\Python311\\Lib\\ast.py:64\u001b[0m in \u001b[0;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mC:\\Python311\\Lib\\ast.py:50\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    return compile(source, filename, mode, flags,\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<unknown>:2\u001b[1;36m\u001b[0m\n\u001b[1;33m    <!doctype html>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# convert this b'{\"chat_history\": [\"test\"]}' to a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=b'{\"chat_history\": [\"test\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn string to json\n",
    "# test=test.decode(\"utf-8\")\n",
    "json_data=json.loads(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[\"chat_history\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
